# Архитектурные паттерны для продвинутой долговременной памяти в агентах на базе Claude 4

## **Краткий обзор**

В данном отчете представлено углубленное исследование передовых и альтернативных методологий реализации долговременной памяти для интеллектуальных агентов, построенных на базе семейства моделей Claude 4. Цель исследования — предоставить архитекторам ИИ всеобъемлющее руководство по выходу за рамки базовых реализаций Retrieval-Augmented Generation (RAG) и изучению более сложных, многоуровневых архитектур.

Ключевые выводы анализа указывают на явный сдвиг индустрии в сторону гибридных RAG-пайплайнов, которые объединяют несколько техник для повышения точности извлечения информации. Также подчеркивается фундаментальное различие между двумя основными функциями памяти: извлечением фактов из внешних баз знаний (задача RAG) и поддержанием контекста текущего диалога (задача суммирования). Отдельное внимание уделяется GraphRAG — подходу, использующему структурированные графы знаний, который представляет собой мощную альтернативу для работы с взаимосвязанными данными.

Основные рекомендации отчета включают в себя принятие фирменной методологии Anthropic «Контекстуальное извлечение» (Contextual Retrieval) в качестве стандарта, использование рекомендованного стека сторонних инструментов (например, Cohere для переранжирования и VoyageAI для эмбеддингов), а также применение гибридной архитектуры памяти, сочетающей RAG и суммирование, для создания по-настоящему адаптивных и многозадачных агентов.

---

## **Часть I: Продвинутые архитектуры Retrieval-Augmented Generation (RAG)**

В этом разделе анализируется эволюция от простых RAG-систем к сложным, многоэтапным конвейерам, разработанным для максимизации релевантности и точности извлечения данных. Мы рассмотрим методы, применяемые как до основного этапа извлечения (предварительная обработка запроса), так и после него (постобработка результатов).

### **Раздел 1.1: Повышение точности через двухэтапное извлечение и переранжирование**

Этот раздел посвящен архитектурному паттерну, который предполагает использование быстрого, но менее точного начального поиска с последующим применением более вычислительно затратного, но высокоточного этапа переранжирования.

### **Ключевая проблема: компромисс между скоростью и точностью при извлечении**

Векторный поиск (dense retrieval), лежащий в основе большинства RAG-систем, быстр и хорошо масштабируется, но может быть неточным. Процесс создания эмбеддингов является сжатием с потерями, что означает, что часть семантического значения текста может быть утрачена.[1] Это приводит к тому, что в контекст, передаваемый большой языковой модели (LLM), попадают нерелевантные документы, что является одной из основных причин генерации "галлюцинаций".[3] Цель продвинутой RAG-архитектуры — максимизировать два показателя: "полноту извлечения" (Retrieval Recall), то есть найти все релевантные документы в базе знаний на первом этапе, и "полноту использования LLM" (LLM Recall), то есть передать модели только самые релевантные из найденных документов для эффективного использования.[1]

### **Двухэтапное архитектурное решение: извлечение и переранжирование**

Для решения этой дилеммы применяется двухэтапная архитектура:

1. **Этап 1: Широкий поиск.** На первом этапе используется быстрый и вычислительно недорогой метод, такой как векторный поиск на основе bi-encoder модели, для извлечения большого набора потенциально релевантных документов (например, топ-20 или топ-50).[1] На этом шаге приоритет отдается полноте, а не точности.
2. **Этап 2: Точное переранжирование.** На втором этапе используется более мощная и вычислительно дорогая модель для пересортировки этого небольшого набора кандидатов на основе их истинной релевантности запросу.[1] Этот второй фильтр отдает приоритет точности.

### **Технический анализ: Cross-Encoders для переранжирования**

Модели-переранжировщики (rerankers) обычно представляют собой cross-encoder модели.1 В отличие от bi-encoder моделей, которые обрабатывают запрос и документ по отдельности для создания их векторных представлений, cross-encoder обрабатывает пару (запрос, документ) *совместно* за один шаг вывода.[1] Эта совместная обработка позволяет модели проводить гораздо более глубокий семантический анализ и улавливать тонкие нюансы взаимосвязи, которые могут быть упущены при сравнении двух независимых векторов.[1] Именно поэтому cross-encoder'ы более точны, но в то же время медленнее и не подходят для прямого поиска по миллионам документов.

### **Реализация с Claude 4: рекомендованный стек Anthropic**

Официальное руководство Anthropic Cookbook наглядно демонстрирует RAG-пайплайн, включающий переранжирование с использованием Cohere Rerank API.[6] Это является весомым подтверждением эффективности данного инструмента для систем на базе Claude. Реализация включает передачу пользовательского запроса и списка изначально найденных документов в эндпоинт Cohere RAG, который возвращает переупорядоченный список с оценками релевантности.[8] Собственные эксперименты Anthropic показывают, что сочетание их метода "Контекстуального извлечения" с переранжированием может сократить количество ошибок извлечения на 67%.[10]

### **[Возможности] и [Ограничения]**

- **[Возможности]:** Значительно повышает релевантность документов, передаваемых Claude 4, что снижает "шум" в контексте и вероятность галлюцинаций. Этот подход позволяет достичь баланса, сочетая скорость векторного поиска с точностью более сложных моделей.[1]
- **[Ограничения]:** Добавляет дополнительную задержку и стоимость в RAG-пайплайн из-за еще одного вызова API к модели переранжирования. Также в общую архитектуру вводится еще один компонент, требующий управления и мониторинга.

### **Архитектурное значение**

Внедрение переранжирования знаменует фундаментальный переход в архитектуре RAG от одноэтапного процесса извлечения к многоэтапному конвейеру. Это признание того, что ни один метод извлечения не является идеальным, и для достижения промышленного качества необходимо наслоение различных техник. Если базовый RAG полагается на однократный векторный поиск, то исследования показывают его несовершенство: векторные эмбеддинги сжимают информацию с потерями и могут не содержать контрастивной информации.[1] Усложнить начальный поиск без потери скорости, которая делает его жизнеспособным, невозможно. Следовательно, сложность должна быть добавлена *после* начального поиска, на гораздо меньшем подмножестве кандидатов. В этом и заключается логика этапа переранжирования. Таким образом, архитектор, проектирующий высококачественную RAG-систему, больше не должен рассматривать одноэтапный поиск как стандарт. Паттерном по умолчанию должен стать двухэтапный конвейер "извлечение-затем-переранжирование". Выбор параметров top_k для начального поиска и top_n для финального переранжированного набора становится критически важным для балансировки стоимости, задержки и качества.

### **Раздел 1.2: Усиление релевантности через трансформацию запроса**

В этом разделе рассматриваются методы предварительной обработки, которые изменяют исходный запрос пользователя *до* этапа извлечения, чтобы сократить семантический разрыв между намерением пользователя и хранящимися данными.

### **Ключевая проблема: несоответствие между запросом и документом**

Пользователи часто задают вопросы, в то время как базы знаний содержат утверждения. Это создает семантическое несоответствие, которое может стать проблемой для моделей эмбеддингов.[11] Запрос "Как работает фотосинтез?" семантически далек от фрагмента документа, который начинается со слов "Фотосинтез — это процесс, посредством которого...". Кроме того, модели эмбеддингов могут плохо обобщаться на новые или специализированные домены, которые не были хорошо представлены в их обучающих данных.[11]

### **Решение: Hypothetical Document Embeddings (HyDE)**

- **Принцип:** Вместо того чтобы создавать эмбеддинг пользовательского запроса, HyDE использует LLM для генерации *гипотетического* документа — вымышленного, идеализированного ответа на запрос.[11]
- **Процесс:**
    1. **Генерация:** По данному запросу LLM (например, Claude 4) генерирует гипотетический ответ. Этот сгенерированный текст улавливает релевантные паттерны, ключевые слова и структуру хорошего ответа.[11]
    2. **Кодирование:** Этот гипотетический документ затем кодируется в векторный эмбеддинг.
    3. **Извлечение:** Полученный вектор используется для поиска в векторной базе данных. Теперь поиск ведется по документам, похожим на *идеальный ответ*, а не на исходный *вопрос*, что значительно повышает точность извлечения.[12]
- **Преимущества:** HyDE отлично работает в zero-shot сценариях, где отсутствуют размеченные данные о релевантности, поскольку он использует генеративную мощь LLM для создания собственного сигнала релевантности.[11] Это помогает снизить вероятность галлюцинаций за счет извлечения более релевантных документов.[12]

### **Реализация и фреймворки**

Теоретическую основу заложила научная статья "Precise Zero-Shot Dense Retrieval without Relevance Labels".[11] Фреймворки, такие как LangChain, предоставляют встроенные модули, например HypotheticalDocumentEmbedder, для легкой интеграции этой техники в RAG-пайплайн.[16] Реализация включает создание цепочки, которая сначала вызывает LLM для генерации гипотетического документа из запроса, а затем использует этот вывод для подачи в систему извлечения.[12]

### **Другие техники трансформации запросов**

Помимо HyDE, существуют и другие методы:

- **Переписывание запроса (Query Rewriting):** Использование LLM для того, чтобы сделать запрос более конкретным или подробным.[18]
- **Запрос "с шагом назад" (Step-Back Prompting):** Генерация более широкого, общего запроса для извлечения более широкого контекста перед тем, как сфокусироваться на исходном конкретном запросе.[18]
- **Декомпозиция подзапросов (Sub-Query Decomposition):** Разбиение сложного запроса на несколько более простых подзапросов, которые выполняются независимо.[18]

### **[Возможности] и [Ограничения]**

- **[Возможности]:** Значительно улучшает извлечение в специализированных или ранее не встречавшихся доменах без необходимости тонкой настройки. Лучше согласовывает вектор поиска с вероятной структурой документов-ответов.
- **[Ограничения]:** Добавляет дополнительный вызов LLM в начале каждого запроса, увеличивая задержку и стоимость. Качество извлечения теперь зависит от качества гипотетически сгенерированного документа, который сам по себе может содержать фактические ошибки (хотя они и не важны для процесса создания эмбеддинга).[14]

### **Архитектурное значение**

HyDE коренным образом переосмысливает задачу извлечения, переходя от "найти документы, похожие на этот вопрос" к "найти документы, похожие на идеальный ответ на этот вопрос". Это мощный когнитивный сдвиг в проектировании систем. Проблема в том, что запросы и документы часто являются семантически разными типами текста. Модели эмбеддингов хорошо находят похожие тексты, но эмбеддинги "вопросов" могут быть не "близки" к эмбеддингам "ответов" в векторном пространстве. Однако LLM отлично справляется с генерацией ответа на вопрос. Таким образом, LLM можно использовать как "переводчик" для преобразования запроса в тот *тип* текста, который мы ищем (ответ). Создавая эмбеддинг этого "переведенного" текста, мы ищем в более релевантной части векторного пространства. Следовательно, архитекторам следует рассматривать модуль "Трансформации запроса" как стандартный компонент своего RAG-пайплайна, особенно для приложений в нишевых доменах. Этот модуль, работающий на быстрой LLM, такой как Claude Haiku, повысит устойчивость системы к плохо сформулированным пользовательским запросам и сдвигу домена.

### **Раздел 1.3: Синтез знаний с помощью GraphRAG**

В этом разделе подробно рассматривается смена парадигмы от извлечения неструктурированных фрагментов текста к запросам к структурированным графам знаний, что позволяет выполнять более сложные рассуждения.

### **Ключевая проблема: фрагментированные знания и реляционная слепота**

Традиционный RAG рассматривает фрагменты (чанки) как изолированные, независимые единицы информации. Ему трудно понять отношения *между* фрагментами или сущностями.[19] Это делает его слабым в "многошаговых" рассуждениях (multi-hop reasoning), где для ответа требуется связать информацию из нескольких источников.[20] Например, ответ на вопрос "Какова рыночная капитализация компании, основанной создателем iPhone?" требует установления цепочки: "создатель iPhone" -> "Стив Джобс" -> "Apple" -> "Рыночная капитализация Apple". Один лишь векторный поиск не может надежно выполнить эти переходы.

### **Решение: GraphRAG**

- **Принцип:** Вместо векторной базы данных с фрагментами текста, GraphRAG использует граф знаний, где информация хранится в виде сети узлов (сущностей) и ребер (отношений).[20]
- **Процесс:**
    1. **Создание графа (индексация):** LLM анализирует неструктурированные документы для извлечения сущностей (например, "Claude 4", "Anthropic") и их отношений (например, "разработан"). Эти структурированные данные загружаются в графовую базу данных, такую как Neo4j.[19]
    2. **Гибридное извлечение:** При поступлении запроса он обрабатывается для выявления сущностей и отношений. Затем система может выполнить гибридный поиск:
        - **Обход графа:** Использование языков графовых запросов (например, Cypher для Neo4j) для перемещения по графу, отслеживания отношений для поиска связанной информации (выполнения многошаговых рассуждений).[20]
        - **Векторный поиск по узлам:** Сами узлы могут иметь векторные эмбеддинги, что позволяет использовать семантический поиск для нахождения правильной отправной точки в графе.[22]
    3. **Дополнение контекста:** Извлеченный подграф и связанный с ним текст передаются LLM в качестве контекста.

### **Ключевые технологии и фреймворки**

- **Microsoft GraphRAG:** Ведущий фреймворк, который формализует этот процесс, сочетая извлечение текста, сетевой анализ и суммирование с помощью LLM.[19]
- **Графовые базы данных:** Neo4j является одним из основных вариантов, предлагая нативные векторные возможности и интеграцию с фреймворками LLM.[22]
- **Векторные базы данных:** Часто используются в тандеме. Например, Qdrant может хранить эмбеддинги для узлов, чтобы инициировать обход графа в Neo4j.[23]

### **[Возможности] и [Ограничения]**

- **[Возможности]:** Превосходно справляется со сложными, многошаговыми вопросами. Обеспечивает высокую объяснимость и "аудиторский след", показывая путь, пройденный по графу для получения ответа.[22] Эффективно моделирует и запрашивает сильно взаимосвязанные данные.
- **[Ограничения]:** Значительные первоначальные затраты и сложность создания и поддержания графа знаний. Качество RAG-системы сильно зависит от качества начального процесса извлечения сущностей и отношений, что может быть сложной задачей.

### **Архитектурное значение**

GraphRAG — это не просто улучшение RAG; это принципиально иной подход к моделированию данных для памяти агента. Он обменивает простоту неструктурированных данных на мощь структурированных знаний. Векторный RAG отлично справляется с вопросами "что" (семантическое сходство), но терпит неудачу с вопросами "как" и "почему", которые включают отношения и причинно-следственные связи. Графы — это основная структура данных для моделирования отношений. Следовательно, чтобы обеспечить реляционные рассуждения, память агента должна быть структурирована как граф. Задача тогда сводится к преобразованию неструктурированного текста в эту графовую структуру, для чего хорошо подходят LLM (извлечение сущностей/отношений). Выбор между векторным RAG и GraphRAG является фундаментальным архитектурным решением. Он полностью зависит от характера базы знаний и типов запросов, которые должен обрабатывать агент. Для базы знаний с взаимосвязанными фактами (например, организационные схемы компаний, зависимости продуктов, научная литература) GraphRAG будет предпочтительнее. Для корпуса независимых документов (например, тикеты поддержки, посты в блогах) более подходит векторный RAG. Очень продвинутая система может использовать оба подхода.

---

### **Таблица 1: Сравнительный анализ продвинутых техник RAG**

| **Техника** | **Основной принцип** | **Наилучшее применение** | **Преимущества** | **Недостатки** | **Интеграция с Claude 4** |
| --- | --- | --- | --- | --- | --- |
| **Переранжирование** | Быстрый, широкий поиск с последующей медленной, точной пересортировкой результатов с помощью cross-encoder модели.[1] | Максимизация точности в финальном наборе документов, отправляемых в LLM. Критически важно для снижения шума и галлюцинаций. | Улучшает использование контекста LLM, предоставляя более качественный и релевантный контекст. Балансирует скорость и точность.[1] | Добавляет задержку и стоимость из-за дополнительного вызова API. Увеличивает сложность конвейера. | Руководство Anthropic явно рекомендует и демонстрирует использование Cohere Rerank API.[6] |
| **HyDE** | Сгенерировать гипотетический "идеальный ответ" на запрос и создать эмбеддинг этого ответа для поиска похожих документов.[11] | Zero-shot извлечение в специализированных доменах, где запросы и документы семантически удалены друг от друга. | Отличное обобщение без размеченных данных. Сокращает разрыв между запросами в виде вопросов и документами в виде ответов.[11] | Добавляет задержку и стоимость из-за начального вызова LLM. Качество извлечения зависит от качества сгенерированного документа. | Может быть реализован с использованием любой модели Claude 4 для генерации. LangChain предоставляет HypotheticalDocumentEmbedder для легкой интеграции.[16] |
| **GraphRAG** | Хранить знания в виде структурированного графа сущностей и отношений и извлекать информацию путем обхода графа.[20] | Ответы на сложные, многошаговые вопросы по сильно взаимосвязанным данным. Обеспечение высокой объяснимости. | Может рассуждать об отношениях. Предоставляет "аудиторский след" для ответов. Преодолевает проблему "фрагментированных знаний" векторного RAG.[19] | Высокие первоначальные затраты и сложность создания и поддержания графа знаний. Производительность зависит от качества извлечения сущностей/отношений. | Может быть интегрирован с Claude 4 в качестве движка для генерации и извлечения сущностей, используя графовую БД, такую как Neo4j, в качестве бэкенда.[22] |

---

## **Часть II: Stateful Summarization как механизм разговорной памяти**

В этой части анализируется альтернативная архитектура памяти, которая фокусируется на поддержании состояния *текущего взаимодействия*, а не на извлечении данных из статической внешней базы знаний.

### **Раздел 2.1: Принцип и практика суммирования с помощью LLM**

### **Ключевая проблема: LLM без состояния и "взрыв" контекста**

LLM по своей сути являются stateless (без состояния); каждый вызов API независим.[29] Для создания связного диалога история должна передаваться с каждым новым запросом.[31] Передача полной, необработанной истории неэффективна. Это приводит к высоким затратам на токены, увеличению задержки и может даже снизить производительность модели, поскольку окно контекста заполняется менее релевантной информацией.[29]

### **Решение: Stateful Summarization (Суммирование с сохранением состояния)**

- **Принцип:** Вместо передачи всей необработанной истории используется LLM для создания непрерывного резюме (summary) диалога. Это сжатое резюме затем передается в качестве "истории" в промпте для следующего шага.[30]
- **Процесс:**
    1. Начинается диалог. Сообщения сохраняются во временном буфере.
    2. Когда буфер превышает определенный лимит токенов, берутся самые старые сообщения.
    3. Вызывается быстрая и экономичная LLM (например, Claude 4 Haiku) для суммирования этих сообщений, возможно, с учетом предыдущего резюме.
    4. Это новое, обновленное резюме заменяет старые сообщения в буфере памяти.
    5. Затем это резюме предоставляется в качестве контекста основной LLM (например, Claude 4 Opus) для генерации следующего ответа.

### **Паттерны реализации и фреймворки**

Это распространенный паттерн для управления тем, что в литературе называется "краткосрочной памятью" или "активной разговорной памятью".[30] LangChain предоставляет несколько классов для этого, таких как ConversationSummaryMemory (суммирует весь диалог) и ConversationSummaryBufferMemory (гибрид, который сохраняет последние сообщения в сыром виде, а более старые суммирует).[32] Они представляют собой готовый архитектурный паттерн для реализации. Резюме может храниться в простом хранилище типа "ключ-значение", например Redis, и быть связано с идентификатором диалога.[30]

### **[Возможности] и [Ограничения]**

- **[Возможности]:** Эффективно управляет ограничениями окна контекста, поддерживая связность диалогов на протяжении длительных взаимодействий. Значительно сокращает количество токенов, что приводит к снижению затрат и задержек по сравнению с передачей полной истории.[33]
- **[Ограничения]:** Процесс суммирования по своей природе является сжатием с потерями; мелкие детали, конкретные формулировки или тонкие факты из разговора могут быть утеряны.[33] Качество памяти зависит от навыков суммирования используемой LLM.

### **Раздел 2.2: Сравнительный анализ: RAG против Stateful Summarization для памяти**

В этом разделе представлено прямое сравнение использования RAG и Stateful Summarization в качестве механизма памяти агента.

### **Определение задачи "памяти"**

Критически важно различать два типа памяти:

- **Память знаний (RAG):** Вспоминание фактов из большого, внешнего и часто статического корпуса информации. Цель — фактическая точность и проверяемость.
- **Разговорная память (Суммирование):** Запоминание того, что было сказано *в рамках текущего взаимодействия*. Цель — повествовательная связность и контекстуальная осведомленность.

### **Сравнительный анализ**

- **Стоимость:**
    - **RAG:** Включает затраты на создание эмбеддингов, хранение в векторной БД и вызовы API для извлечения. Стоимость вывода пропорциональна размеру извлеченных фрагментов. Может быть оптимизирована с помощью таких методов, как бюджетирование токенов.[37]
    - **Суммирование:** Включает стоимость вызова LLM для каждого шага суммирования. Стоимость пропорциональна длине суммируемого текста. В целом дешевле, чем передача полной истории, но дороже, чем простое буферизирование.[33]
- **Задержка:**
    - **RAG:** Задержка в основном определяется временем извлечения из векторной/графовой базы данных. Может быть высокой, если база данных не оптимизирована.[38]
    - **Суммирование:** Задержка в основном определяется вызовом LLM для генерации резюме. Критически важно использовать быструю модель, такую как Haiku.
- **Качество и глубина информации:**
    - **RAG:** Предоставляет высокоточные, дословные фрагменты информации. Отлично подходит для фактической точности. Однако RAG является плохим инструментом для общей задачи суммирования, так как он извлекает только конкретные фрагменты, а не суть всего корпуса.[39]
    - **Суммирование:** Предоставляет дистиллированный, абстрактный обзор. Отлично подходит для поддержания потока разговора, но теряет конкретные детали и не является дословным.[33] Само резюме является сгенерированным артефактом и может содержать неточности.

### **Архитектурное значение**

RAG и Stateful Summarization — это не конкурирующие технологии; это взаимодополняющие компоненты комплексной системы памяти. Они решают разные проблемы. Агенту нужно помнить две вещи: что он *знает* (внешние знания) и что было *сказано* (история разговора). RAG — это механизм доступа к тому, что он знает. Это как "экзамен с открытой книгой". Суммирование — это механизм запоминания того, что было сказано. Это как ведение протокола встречи. Попытка использовать RAG для разговорной памяти (например, векторизация каждого сообщения) неэффективна и может упустить повествовательную линию. Попытка использовать суммирование для памяти знаний невозможна. Следовательно, по-настоящему продвинутый агент требует гибридной архитектуры памяти. Он должен использовать RAG для извлечения релевантных фактов из своей базы знаний при необходимости и использовать Stateful Summarization для управления контекстом непосредственного разговора. Финальный промпт, подаваемый основной LLM, будет дополнен *как* извлеченным RAG-контекстом, *так и* разговорным резюме.

---

### **Таблица 2: RAG против Stateful Summarization для разговорной памяти**

| **Критерий** | **RAG (для памяти)** | **Stateful Summarization** | **Архитектурные примечания** |
| --- | --- | --- | --- |
| **Основная функция** | Извлечение конкретных, дословных фактов из прошлых взаимодействий/документов. | Поддержание непрерывного, связного повествования текущего разговора. | RAG для *извлечения знаний*; Суммирование для *управления состоянием*. |
| **Качество информации** | Высокоточное, проверяемое. Может упускать более широкий контекст или повествовательную линию.[33] | Абстрактное, улавливает "суть". По своей природе сжимает с потерями, конкретные детали могут быть утеряны.[33] | Используйте RAG для "Что вы говорили о X?" и Суммирование для "Основываясь на нашем обсуждении...". |
| **Профиль затрат** | Затраты на начальную индексацию. Стоимость извлечения за запрос. Стоимость вывода на извлеченных фрагментах.[37] | Стоимость вызова LLM за каждое суммирование. Стоимость растет с длиной разговора, но меньше, чем у полной истории.[33] | Для суммирования можно использовать более дешевую модель (Haiku), в то время как извлечение RAG имеет фиксированную стоимость за запрос. |
| **Профиль задержки** | Определяется временем извлечения из базы данных. Может быть субсекундным с оптимизированными векторными БД.[38] | Определяется вызовом LLM для генерации резюме. Зависит от скорости модели. | Задержка RAG связана с вводом-выводом; задержка Суммирования — с вычислениями. |
| **Масштабируемость** | Масштабируется до миллиардов документов в базе знаний (например, Pinecone, Weaviate).[41] | Масштабируется до очень длинных разговоров, но само резюме может стать узким местом, если станет слишком большим. | RAG масштабируется с размером базы знаний; Суммирование — с длиной одного взаимодействия. |
| **Реализация** | Требует настройки векторной/графовой БД, модели эмбеддингов и конвейера извлечения.[43] | Может быть реализовано с помощью LLM и хранилища ключ-значение (например, Redis) или с использованием фреймворков, таких как LangChain Memory.[30] | RAG более требователен к инфраструктуре; Суммирование — к логике внутри приложения. |

---

## **Часть III: Интегрированный план для систем памяти Claude 4**

В этой заключительной части синтезируется предыдущий анализ в набор конкретных архитектурных паттернов и рекомендаций, специально разработанных для создания агентов с использованием семейства моделей Claude 4.

### **Раздел 3.1: Использование экосистемы Anthropic и рекомендованного стека**

### **"Контекстуальное извлечение" от Anthropic как лучшая практика**

Это фирменный вклад Anthropic в архитектуру RAG.[10] Техника заключается в использовании LLM (например, Claude Haiku) для добавления объяснительного контекста к каждому фрагменту *перед* созданием эмбеддинга и индексацией.[10] Это напрямую решает проблему "потерянного контекста" при фрагментации и, как было показано, сокращает количество неудачных извлечений на 49%.[10] Архитектурно это предполагает, что конвейер "Индексации с помощью LLM" должен стать паттерном по умолчанию для высокопроизводительного RAG на базе Claude. Стоимость этого процесса становится управляемой благодаря функции кэширования промптов от Anthropic, что создает мощную синергию внутри их экосистемы.[10]

### **Рекомендованный набор сторонних инструментов**

- **Модели эмбеддингов:** Anthropic не предоставляет собственные модели эмбеддингов.[6] Их официальное руководство последовательно использует
    
    **VoyageAI** для генерации высококачественных эмбеддингов.[6] Это является сильным сигналом для рекомендации этого компонента.
    
- **Модели переранжирования:** В руководстве явно используется модель **Cohere Rerank** в примерах продвинутого RAG, что демонстрирует ее эффективность и простоту интеграции.[6]
- **Векторные базы данных:** Руководство и другие ресурсы показывают интеграцию с различными стандартными векторными базами данных, включая **Pinecone** [46], **ChromaDB** [48] и **Weaviate** [51], что указывает на гибкость в этой части стека. Выбор зависит от масштаба, стоимости и предпочтения управляемого сервиса.[41]

### **Специфика Claude 4: большие окна контекста и многоагентные системы**

Окно контекста Claude 4 в 200k токенов меняет расчеты для RAG. Это делает такие техники, как "stuffing" (прямая подача всего текста), более жизнеспособными для суммирования и позволяет использовать более крупные, богатые контекстом фрагменты в RAG.[49] Anthropic активно движется в сторону многоагентных систем, где ведущий агент (Opus 4) делегирует подзадачи другим агентам (Sonnet 4).[55] Это продвинутая форма RAG, где сами агенты становятся инструментами для извлечения и синтеза информации.

### **Раздел 3.2: Фреймворк для принятия решений по архитектуре памяти**

Этот раздел представляет собой практическое, основанное на сценариях руководство для архитекторов по выбору правильной архитектуры памяти.

- **Сценарий 1: Простой агент для ответов на вопросы на основе фактов (например, бот поддержки для документации по продукту).**
    - **Рекомендуемая архитектура:** Базовый RAG-пайплайн с "Контекстуальным извлечением" от Anthropic.
    - **Обоснование:** Основная потребность — точное извлечение фактов. "Контекстуальное извлечение" обеспечивает значительное повышение качества при однократных затратах на индексацию. Переранжирование можно добавить, если точность является первостепенной.
- **Сценарий 2: Ассистент-эксперт для исследований (например, анализ научных статей, юридических документов).**
    - **Рекомендуемая архитектура:** Продвинутый RAG-пайплайн ("Контекстуальное извлечение" + HyDE + Переранжирование).
    - **Обоснование:** Запросы сложные и относятся к специализированной области. HyDE поможет преодолеть разрыв между запросом и документом, а переранжирование обеспечит, что пользователю будут представлены только самые релевантные доказательства.
- **Сценарий 3: Аналитик бизнес-аналитики (например, запросы к взаимосвязанным данным компании).**
    - **Рекомендуемая архитектура:** GraphRAG.
    - **Обоснование:** Данные являются реляционными (сотрудники, отделы, проекты). Запросы, скорее всего, будут многошаговыми. GraphRAG специально создан для этого, предоставляя объяснимые и точные ответы на основе связанных данных.
- **Сценарий 4: Долгосрочный творческий соавтор или личный ассистент.**
    - **Рекомендуемая архитектура:** Гибридная система памяти (Stateful Summarization + RAG).
    - **Обоснование:** Этому агенту нужна как разговорная память (чтобы помнить цели пользователя, стиль и прошлые разговоры), так и память знаний (чтобы извлекать факты о мире). Stateful Summarization будет управлять потоком разговора, а RAG будет использоваться как инструмент, когда агенту нужно "что-то посмотреть".

### **Раздел 3.3: Заключительные архитектурные рекомендации и перспективы**

### **Краткое изложение ключевых паттернов:**

1. **Применяйте многоэтапный RAG:** Выходите за рамки наивного RAG. Базовый уровень качества должен включать предварительную обработку ("Контекстуальное извлечение", HyDE) и постобработку (Переранжирование).
2. **Выбирайте правильную модель данных:** Выбор между векторным RAG и GraphRAG является фундаментальным решением, основанным на структуре вашей базы знаний.
3. **Реализуйте гибридную память:** Для сложных, интерактивных агентов разделяйте и комбинируйте разговорную память (Суммирование) и память знаний (RAG).

### **"Стек Claude 4":**

Для большинства высококачественных сценариев использования RAG рекомендуемая архитектура выглядит следующим образом:

- **Индексация:** Используйте Claude Haiku для "Контекстуального извлечения" от Anthropic для обогащения фрагментов.
- **Эмбеддинг:** Используйте модель высшего уровня, такую как VoyageAI.
- **Хранение:** Используйте масштабируемую векторную БД, такую как Pinecone или Weaviate.
- **Извлечение:** Комбинируйте векторный поиск с последующим переранжированием от Cohere.
- **Генерация:** Используйте Claude 4 Sonnet или Opus.

### **Перспективы на будущее**

Продолжающееся расширение окон контекста будет и дальше стирать границы, делая гибридные подходы еще более мощными. Развитие многоагентных фреймворков будет рассматривать RAG не просто как конвейер, а как динамическую, делегируемую задачу, как показано в собственных исследованиях Anthropic.55 Конечная цель — создание агентов, которые могут бесшовно интегрировать свое внутреннее разговорное состояние с обширной, структурированной и мгновенно доступной внешней базой знаний.

---

### **Таблица 3: Рекомендованные инструменты для реализации RAG с Claude 4**

| **Компонент** | **Рекомендованный инструмент(ы)** | **Обоснование / Источник(и)** |
| --- | --- | --- |
| **Генеративная LLM** | Claude 4 Opus, Claude 4 Sonnet | Основные модели для высококачественной генерации. Opus для максимальных возможностей, Sonnet для сбалансированной производительности.[56] |
| **LLM для контекстуализации** | Claude 4 Haiku | Для шагов предварительной обработки с помощью LLM, таких как "Контекстуальное извлечение" или HyDE, идеально подходит быстрая и экономичная модель.[10] |
| **Модель эмбеддингов** | VoyageAI | Anthropic не предлагает собственные модели эмбеддингов. Их официальное руководство последовательно использует и рекомендует VoyageAI для этой цели.[6] |
| **Модель переранжирования** | Cohere Rerank | Явно используется и рекомендуется в руководстве Anthropic для этапа переранжирования в продвинутом RAG-пайплайне.[6] |
| **Векторная база данных** | Pinecone, Weaviate, ChromaDB | Множество вариантов жизнеспособны и показаны в руководствах. Выбор зависит от масштаба и предпочтений в управлении (управляемый сервис против open-source).[41] |
| **Графовая база данных** | Neo4j | Ведущий выбор для реализаций GraphRAG, с сильной поддержкой сообщества и интеграциями с фреймворками LLM.[22] |

### **Источники**

1. Improving RAG Performance: WTF are Re-Ranking Techniques ..., дата последнего обращения: июля 5, 2025, [https://www.fuzzylabs.ai/blog-post/improving-rag-performance-re-ranking](https://www.fuzzylabs.ai/blog-post/improving-rag-performance-re-ranking)
2. What are Rerankers? - MongoDB, дата последнего обращения: июля 5, 2025, [https://www.mongodb.com/resources/basics/artificial-intelligence/reranking-models](https://www.mongodb.com/resources/basics/artificial-intelligence/reranking-models)
3. Mastering RAG: How to Select A Reranking Model - Galileo AI, дата последнего обращения: июля 5, 2025, [https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model](https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model)
4. RAG techniques: From naive to advanced - Weights & Biases - Wandb, дата последнего обращения: июля 5, 2025, [https://wandb.ai/site/articles/rag-techniques/](https://wandb.ai/site/articles/rag-techniques/)
5. docs.zenml.io, дата последнего обращения: июля 5, 2025, [https://docs.zenml.io/user-guides/llmops-guide/reranking/understanding-reranking#:~:text=how%20reranking%20works.-,What%20is%20reranking%3F,to%20generate%20the%20final%20output.](https://docs.zenml.io/user-guides/llmops-guide/reranking/understanding-reranking#:~:text=how%20reranking%20works.-,What%20is%20reranking%3F,to%20generate%20the%20final%20output.)
6. anthropic-cookbook/skills/contextual-embeddings/guide.ipynb at main - GitHub, дата последнего обращения: июля 5, 2025, [https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb)
7. Anthropic-cookbook:Skills:Contextual-embeddings:Guide.ipynb at Main - Scribd, дата последнего обращения: июля 5, 2025, [https://www.scribd.com/document/805515099/Anthropic-cookbook-Skills-Contextual-embeddings-Guide-ipynb-at-Main-Anthropics](https://www.scribd.com/document/805515099/Anthropic-cookbook-Skills-Contextual-embeddings-Guide-ipynb-at-Main-Anthropics)
8. Reranking - quickstart - Cohere Documentation, дата последнего обращения: июля 5, 2025, [https://docs.cohere.com/v2/docs/reranking-quickstart](https://docs.cohere.com/v2/docs/reranking-quickstart)
9. Master Reranking with Cohere Models | Cohere, дата последнего обращения: июля 5, 2025, [https://docs.cohere.com/docs/reranking-with-cohere](https://docs.cohere.com/docs/reranking-with-cohere)
10. Introducing Contextual Retrieval \ Anthropic, дата последнего обращения: июля 5, 2025, [https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)
11. Hypothetical Document Embeddings (HyDE), дата последнего обращения: июля 5, 2025, [https://docs.haystack.deepset.ai/docs/hypothetical-document-embeddings-hyde](https://docs.haystack.deepset.ai/docs/hypothetical-document-embeddings-hyde)
12. Implementing RAG using Hypothetical Document Embeddings (HyDE), дата последнего обращения: июля 5, 2025, [https://hub.athina.ai/athina-originals/implementing-rag-using-hypothetical-document-embeddings-hyde/](https://hub.athina.ai/athina-originals/implementing-rag-using-hypothetical-document-embeddings-hyde/)
13. A Complete Guide to Implementing HyDE RAG | by Gaurav Nigam | aingineer - Medium, дата последнего обращения: июля 5, 2025, [https://medium.com/aingineer/a-complete-guide-to-implementing-hyde-rag-82492551f3d8](https://medium.com/aingineer/a-complete-guide-to-implementing-hyde-rag-82492551f3d8)
14. RAG with Hypothetical Document Embeddings(HyDE) | by TeeTracker - Medium, дата последнего обращения: июля 5, 2025, [https://teetracker.medium.com/rag-with-hypothetical-document-embeddings-hyde-0edeca23f891](https://teetracker.medium.com/rag-with-hypothetical-document-embeddings-hyde-0edeca23f891)
15. Enhancing RAG Performance Using Hypothetical Document Embeddings (HyDE), дата последнего обращения: июля 5, 2025, [https://coralogix.com/ai-blog/enhancing-rag-performance-using-hypothetical-document-embeddings-hyde/](https://coralogix.com/ai-blog/enhancing-rag-performance-using-hypothetical-document-embeddings-hyde/)
16. Enhancing RAG with Hypothetical Document Embedding - Analytics Vidhya, дата последнего обращения: июля 5, 2025, [https://www.analyticsvidhya.com/blog/2024/04/enhancing-rag-with-hypothetical-document-embedding/](https://www.analyticsvidhya.com/blog/2024/04/enhancing-rag-with-hypothetical-document-embedding/)
17. langchain.chains.hyde.base.HypotheticalDocumentEmbedder, дата последнего обращения: июля 5, 2025, [https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html](https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html)
18. all-rag-techniques/7_query_transform.ipynb at main - GitHub, дата последнего обращения: июля 5, 2025, [https://github.com/FareedKhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb](https://github.com/FareedKhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb)
19. How Would Microsoft GraphRAG Work Alongside a Graph Database?, дата последнего обращения: июля 5, 2025, [https://memgraph.com/blog/how-microsoft-graphrag-works-with-graph-databases](https://memgraph.com/blog/how-microsoft-graphrag-works-with-graph-databases)
20. What is GraphRAG? | IBM, дата последнего обращения: июля 5, 2025, [https://www.ibm.com/think/topics/graphrag](https://www.ibm.com/think/topics/graphrag)
21. Vector database vs. graph database: Knowledge Graph impact - WRITER, дата последнего обращения: июля 5, 2025, [https://writer.com/engineering/vector-database-vs-graph-database/](https://writer.com/engineering/vector-database-vs-graph-database/)
22. Generative AI - Ground LLMs with Knowledge Graphs - Neo4j, дата последнего обращения: июля 5, 2025, [https://neo4j.com/generativeai/](https://neo4j.com/generativeai/)
23. GraphRAG with Qdrant and Neo4j, дата последнего обращения: июля 5, 2025, [https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)
24. User Guide: RAG — neo4j-graphrag-python documentation, дата последнего обращения: июля 5, 2025, [https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html](https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html)
25. www.microsoft.com, дата последнего обращения: июля 5, 2025, [https://www.microsoft.com/en-us/research/project/graphrag/#:~:text=GraphRAG%20(Graphs%20%2B%20Retrieval%20Augmented%20Generation,end%2Dto%2Dend%20system.](https://www.microsoft.com/en-us/research/project/graphrag/#:~:text=GraphRAG%20(Graphs%20%2B%20Retrieval%20Augmented%20Generation,end%2Dto%2Dend%20system.)
26. microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system, дата последнего обращения: июля 5, 2025, [https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)
27. Getting started with Neo4j GraphRag | Tutorial:1 - YouTube, дата последнего обращения: июля 5, 2025, [https://www.youtube.com/watch?v=1SHJBPiTJ6Y](https://www.youtube.com/watch?v=1SHJBPiTJ6Y)
28. how to use Neo4j graphRAG with Claude MCP - YouTube, дата последнего обращения: июля 5, 2025, [https://www.youtube.com/watch?v=i029kVz6DqA](https://www.youtube.com/watch?v=i029kVz6DqA)
29. Memory and State in LLM Applications - Arize AI, дата последнего обращения: июля 5, 2025, [https://arize.com/blog/memory-and-state-in-llm-applications/](https://arize.com/blog/memory-and-state-in-llm-applications/)
30. Build smarter AI agents: Manage short-term and long-term memory ..., дата последнего обращения: июля 5, 2025, [https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/)
31. Enhancing LLM's Conversations with Efficient Summarization - foojay, дата последнего обращения: июля 5, 2025, [https://foojay.io/today/summarizingtokenwindowchatmemory-enhancing-llms-conversations-with-efficient-summarization/](https://foojay.io/today/summarizingtokenwindowchatmemory-enhancing-llms-conversations-with-efficient-summarization/)
32. 03-langchain-conversational-memory.ipynb - Colab, дата последнего обращения: июля 5, 2025, [https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb)
33. How Should I Manage Memory for my LLM Chatbot? - Vellum AI, дата последнего обращения: июля 5, 2025, [https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot](https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot)
34. Conversational Memory in LangChain | Aurelio AI, дата последнего обращения: июля 5, 2025, [https://www.aurelio.ai/learn/langchain-conversational-memory](https://www.aurelio.ai/learn/langchain-conversational-memory)
35. Conversational Memory for LLMs with Langchain - Pinecone, дата последнего обращения: июля 5, 2025, [https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)
36. Summarizing past messages in an RAG conversation - is it always recommended? : r/LangChain - Reddit, дата последнего обращения: июля 5, 2025, [https://www.reddit.com/r/LangChain/comments/1am5nsd/summarizing_past_messages_in_an_rag_conversation/](https://www.reddit.com/r/LangChain/comments/1am5nsd/summarizing_past_messages_in_an_rag_conversation/)
37. Reducing Latency and Cost at Scale: How Leading Enterprises Optimize LLM Performance, дата последнего обращения: июля 5, 2025, [https://www.tribe.ai/applied-ai/reducing-latency-and-cost-at-scale-llm-performance](https://www.tribe.ai/applied-ai/reducing-latency-and-cost-at-scale-llm-performance)
38. RAG vs Fine-Tuning : Balancing Cost, Accuracy, and Latency in AI ..., дата последнего обращения: июля 5, 2025, [https://medium.com/@ayushgoelar/rag-vs-fine-tuning-balancing-cost-accuracy-and-latency-in-ai-optimization-d8ba73aff616](https://medium.com/@ayushgoelar/rag-vs-fine-tuning-balancing-cost-accuracy-and-latency-in-ai-optimization-d8ba73aff616)
39. Why RAG Fails at Summarization - Blueteam AI, дата последнего обращения: июля 5, 2025, [https://blueteam.ai/blog/why-rag-fails-at-summarization/](https://blueteam.ai/blog/why-rag-fails-at-summarization/)
40. RAG for summarizing? : r/LocalLLaMA - Reddit, дата последнего обращения: июля 5, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1d2t15z/rag_for_summarizing/](https://www.reddit.com/r/LocalLLaMA/comments/1d2t15z/rag_for_summarizing/)
41. Top 5 Vector Databases to Try in 2024 - Cody, дата последнего обращения: июля 5, 2025, [https://meetcody.ai/blog/top-vector-databases/](https://meetcody.ai/blog/top-vector-databases/)
42. Top 10 Vector Databases in 2024 - LLM & Langchain Blogs, дата последнего обращения: июля 5, 2025, [https://www.langchain.ca/blog/top-10-vector-databases-2024/](https://www.langchain.ca/blog/top-10-vector-databases-2024/)
43. What is RAG? - Retrieval-Augmented Generation AI Explained - AWS, дата последнего обращения: июля 5, 2025, [https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
44. Contextual Retrieval with Milvus - Colab - Google, дата последнего обращения: июля 5, 2025, [https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/tutorials/quickstart/contextual_retrieval_with_milvus.ipynb](https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/tutorials/quickstart/contextual_retrieval_with_milvus.ipynb)
45. Getting Started with Retrieval-Augmented Generation - Cohere, дата последнего обращения: июля 5, 2025, [https://cohere.com/llmu/rag-start](https://cohere.com/llmu/rag-start)
46. Claude 3 Opus RAG Chatbot (Full Walkthrough) - YouTube, дата последнего обращения: июля 5, 2025, [https://www.youtube.com/watch?v=rbzYZLfQbAM](https://www.youtube.com/watch?v=rbzYZLfQbAM)
47. anthropics/anthropic-cookbook: A collection of notebooks/recipes showcasing some fun and effective ways of using Claude. - GitHub, дата последнего обращения: июля 5, 2025, [https://github.com/anthropics/anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook)
48. Anthropic Mcp - Chroma Docs, дата последнего обращения: июля 5, 2025, [https://docs.trychroma.com/integrations/frameworks/anthropic-mcp](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp)
49. Building a RAG pipeline using LLamaIndex and Claude 3 | ml-articles - Wandb, дата последнего обращения: июля 5, 2025, [https://wandb.ai/mostafaibrahim17/ml-articles/reports/Building-a-RAG-pipeline-using-LLamaIndex-and-Claude-3--Vmlldzo3NTM4MzYz](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Building-a-RAG-pipeline-using-LLamaIndex-and-Claude-3--Vmlldzo3NTM4MzYz)
50. Claude 3 Quickstart - TruLens, дата последнего обращения: июля 5, 2025, [https://www.trulens.org/cookbook/models/anthropic/claude3_quickstart/](https://www.trulens.org/cookbook/models/anthropic/claude3_quickstart/)
51. Generative AI | Weaviate, дата последнего обращения: июля 5, 2025, [https://weaviate.io/developers/weaviate/model-providers/anthropic/generative](https://weaviate.io/developers/weaviate/model-providers/anthropic/generative)
52. Comparing Chroma DB, Weaviate, and Pinecone: Which Vector Database is Right for You?, дата последнего обращения: июля 5, 2025, [https://medium.com/@rohitupadhye799/comparing-chroma-db-weaviate-and-pinecone-which-vector-database-is-right-for-you-3b85b561b3a3](https://medium.com/@rohitupadhye799/comparing-chroma-db-weaviate-and-pinecone-which-vector-database-is-right-for-you-3b85b561b3a3)
53. The 7 Best Vector Databases in 2025 - DataCamp, дата последнего обращения: июля 5, 2025, [https://www.datacamp.com/blog/the-top-5-vector-databases](https://www.datacamp.com/blog/the-top-5-vector-databases)
54. Towards Long Context RAG — LlamaIndex - Build Knowledge Assistants over your Enterprise Data, дата последнего обращения: июля 5, 2025, [https://www.llamaindex.ai/blog/towards-long-context-rag](https://www.llamaindex.ai/blog/towards-long-context-rag)
55. How we built our multi-agent research system - Anthropic, дата последнего обращения: июля 5, 2025, [https://www.anthropic.com/engineering/built-multi-agent-research-system](https://www.anthropic.com/engineering/built-multi-agent-research-system)
56. Models overview - Anthropic API, дата последнего обращения: июля 5, 2025, [https://docs.anthropic.com/en/docs/about-claude/models/overview](https://docs.anthropic.com/en/docs/about-claude/models/overview)

Introducing Claude 4 - Anthropic, дата последнего обращения: июля 5, 2025,

[https://www.anthropic.com/news/claude-4](https://www.anthropic.com/news/claude-4)